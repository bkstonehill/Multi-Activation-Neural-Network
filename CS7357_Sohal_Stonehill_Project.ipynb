{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "243fa3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Braden\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Braden\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Braden\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Braden\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# Import needed libraries\n",
    "%reload_ext tensorboard\n",
    "\n",
    "import datetime\n",
    "\n",
    "# Data containers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data Manipulation and Evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Deep_Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# This code snippet forces tensorflow to not automatically allocate all GPU ram which can be an issue in notebook environment\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "        \n",
    "tf.debugging.disable_traceback_filtering()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ddb706",
   "metadata": {},
   "source": [
    "### Define the Custom Layer for multiple activation functions per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3be6a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom Layer\n",
    "class MultiActivationLayer(keras.layers.Layer):\n",
    "    '''\n",
    "    Multiple Activation Layer\n",
    "    \n",
    "    A neural network layer in which every node has a different activation function applied\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, out_features, activations, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.out_features = out_features\n",
    "        self.activations = activations\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.w = tf.Variable(tf.random.normal([input_shape[-1], self.out_features]), name='w')\n",
    "        self.b = tf.Variable(tf.zeros([self.out_features]), name='b')\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        z = tf.matmul(inputs, self.w) + self.b\n",
    "        shape = tf.shape(z)[0]\n",
    "        \n",
    "        # Apply activation function to ouput features from nodes (columns) separately with different activation functions\n",
    "        #, reshape to 2-D array and concatenate the results from each node in the same order\n",
    "        nodes = [tf.reshape(self.activations[i%len(self.activations)](z[:,i]), (shape, 1)) for i in range(self.out_features)]\n",
    "        z = tf.concat(nodes, 1)\n",
    "        return z\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"out_features\": self.out_features,\n",
    "            \"activations\": self.activations,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c3f711",
   "metadata": {},
   "source": [
    "## Define method for creating the testing models with equivalent architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2799d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom architecture using the new layer\n",
    "def create_model(activations, option='multi', num_classes=1, dropout=False, dropout_rate=0.2, task='classification'):\n",
    "    if option == 'uniform':\n",
    "        if not dropout:\n",
    "            if task == 'classification':\n",
    "                return keras.models.Sequential([\n",
    "                    keras.layers.Dense(25, activation=activations[0], name='layers_dense_1'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(20, activation=activations[0], name='layers_dense_2'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(15, activation=activations[0], name='layers_dense_3'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(10, activation=activations[0], name='layers_dense_4'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(5, activation=activations[0], name='layers_dense_5'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(num_classes, activation='sigmoid' if num_classes < 2 else 'softmax', name='layers_dense')\n",
    "                ])\n",
    "            else:\n",
    "                 return keras.models.Sequential([\n",
    "                    keras.layers.Dense(25, activation=activations[0], name='layers_dense_1'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(20, activation=activations[0], name='layers_dense_2'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(15, activation=activations[0], name='layers_dense_3'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(10, activation=activations[0], name='layers_dense_4'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(5, activation=activations[0], name='layers_dense_5'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(1, name='layers_dense')\n",
    "                ])\n",
    "        else:\n",
    "            if task == 'classification':\n",
    "                return keras.models.Sequential([\n",
    "                    keras.layers.Dense(25, activation=activations[0], name='layers_dense_1'),\n",
    "                    keras.layers.Dropout(dropout_rate),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(20, activation=activations[0], name='layers_dense_2'),\n",
    "                    keras.layers.Dropout(dropout_rate),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(15, activation=activations[0], name='layers_dense_3'),\n",
    "                    keras.layers.Dropout(dropout_rate),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(10, activation=activations[0], name='layers_dense_4'),\n",
    "                    keras.layers.Dropout(dropout_rate),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(5, activation=activations[0], name='layers_dense_5'),\n",
    "                    keras.layers.Dropout(dropout_rate),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(num_classes, activation='sigmoid' if num_classes < 2 else 'softmax', name='layers_dense')\n",
    "                ])\n",
    "            else:\n",
    "                return keras.models.Sequential([\n",
    "                    keras.layers.Dense(25, activation=activations[0], name='layers_dense_1'),\n",
    "                    keras.layers.Dropout(dropout_rate),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(20, activation=activations[0], name='layers_dense_2'),\n",
    "                    keras.layers.Dropout(dropout_rate),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(15, activation=activations[0], name='layers_dense_3'),\n",
    "                    keras.layers.Dropout(dropout_rate),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(10, activation=activations[0], name='layers_dense_4'),\n",
    "                    keras.layers.Dropout(dropout_rate),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(5, activation=activations[0], name='layers_dense_5'),\n",
    "                    keras.layers.Dropout(dropout_rate),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(1, name='layers_dense')\n",
    "                ])\n",
    "    elif option == 'multi':\n",
    "        if not dropout:\n",
    "            if task == 'classification':\n",
    "                return keras.models.Sequential([\n",
    "                    MultiActivationLayer(25, activations, name='layers_multi_1'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    MultiActivationLayer(20, activations, name='layers_multi_2'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    MultiActivationLayer(15, activations, name='layers_multi_3'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    MultiActivationLayer(10, activations, name='layers_multi_4'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    MultiActivationLayer(5, activations, name='layers_multi_5'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(num_classes, activation='sigmoid' if num_classes < 2 else 'softmax', name='layers_dense')\n",
    "                ])\n",
    "            else:\n",
    "                 return keras.models.Sequential([\n",
    "                    MultiActivationLayer(25, activations, name='layers_multi_1'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    MultiActivationLayer(20, activations, name='layers_multi_2'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    MultiActivationLayer(15, activations, name='layers_multi_3'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    MultiActivationLayer(10, activations, name='layers_multi_4'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    MultiActivationLayer(5, activations, name='layers_multi_5'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(1, name='layers_dense')\n",
    "                ])\n",
    "        else:\n",
    "            if task == 'classification':\n",
    "                return keras.models.Sequential([\n",
    "                    MultiActivationLayer(25, activations, name='layers_multi_1'),\n",
    "                    keras.layers.Dropout(dropout_rate),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    MultiActivationLayer(20, activations, name='layers_multi_2'),\n",
    "                    keras.layers.Dropout(dropout_rate),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    MultiActivationLayer(15, activations, name='layers_multi_3'),\n",
    "                    keras.layers.Dropout(dropout_rate),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    MultiActivationLayer(10, activations, name='layers_multi_4'),\n",
    "                    keras.layers.Dropout(dropout_rate),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    MultiActivationLayer(5, activations, name='layers_multi_5'),\n",
    "                    keras.layers.Dropout(dropout_rate),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(num_classes, activation='sigmoid' if num_classes < 2 else 'softmax', name='layers_dense')\n",
    "                ])\n",
    "            else:\n",
    "                return keras.models.Sequential([\n",
    "                    MultiActivationLayer(25, activations, name='layers_multi_1'),\n",
    "                    keras.layers.Dropout(dropout_rate),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    MultiActivationLayer(20, activations, name='layers_multi_2'),\n",
    "                    keras.layers.Dropout(dropout_rate),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    MultiActivationLayer(15, activations, name='layers_multi_3'),\n",
    "                    keras.layers.Dropout(dropout_rate),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    MultiActivationLayer(10, activations, name='layers_multi_4'),\n",
    "                    keras.layers.Dropout(dropout_rate),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    MultiActivationLayer(5, activations, name='layers_multi_5'),\n",
    "                    keras.layers.Dropout(dropout_rate),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(1, name='layers_dense')\n",
    "                ])\n",
    "    elif option == 'sequential':\n",
    "        if len(activations) < 5: raise RuntimeError()\n",
    "        if not dropout:\n",
    "            if task == 'classification':\n",
    "                return keras.models.Sequential([\n",
    "                    keras.layers.Dense(25, activation=activations[0], name='layers_dense_1'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(20, activation=activations[1], name='layers_dense_2'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(15, activation=activations[2], name='layers_dense_3'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(10, activation=activations[3], name='layers_dense_4'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(5, activation=activations[4], name='layers_dense_5'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(num_classes, activation='sigmoid' if num_classes < 2 else 'softmax', name='layers_dense')\n",
    "                ])\n",
    "            else:\n",
    "                 return keras.models.Sequential([\n",
    "                    keras.layers.Dense(25, activation=activations[0], name='layers_dense_1'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(20, activation=activations[1], name='layers_dense_2'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(15, activation=activations[2], name='layers_dense_3'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(10, activation=activations[3], name='layers_dense_4'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(5, activation=activations[4], name='layers_dense_5'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(1, name='layers_dense')\n",
    "                ])\n",
    "        else: \n",
    "            if task == 'classification':\n",
    "                return keras.models.Sequential([\n",
    "                    keras.layers.Dense(25, activation=activations[0], name='layers_dense_1'),\n",
    "                    keras.layers.Dropout(dropout_rate),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(20, activation=activations[1], name='layers_dense_2'),\n",
    "                    keras.layers.Dropout(dropout_rate),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(15, activation=activations[2], name='layers_dense_3'),\n",
    "                    keras.layers.Dropout(dropout_rate),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(10, activation=activations[3], name='layers_dense_4'),\n",
    "                    keras.layers.Dropout(dropout_rate),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(5, activation=activations[4], name='layers_dense_5'),\n",
    "                    keras.layers.Dropout(dropout_rate),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(num_classes, activation='sigmoid' if num_classes < 2 else 'softmax', name='layers_dense')\n",
    "                ])\n",
    "            else:\n",
    "                return keras.models.Sequential([\n",
    "                    keras.layers.Dense(25, activation=activations[0], name='layers_dense_1'),\n",
    "                    keras.layers.Dropout(dropout_rate),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(20, activation=activations[1], name='layers_dense_2'),\n",
    "                    keras.layers.Dropout(dropout_rate),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(15, activation=activations[2], name='layers_dense_3'),\n",
    "                    keras.layers.Dropout(dropout_rate),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(10, activation=activations[3], name='layers_dense_4'),\n",
    "                    keras.layers.Dropout(dropout_rate),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(5, activation=activations[4], name='layers_dense_5'),\n",
    "                    keras.layers.Dropout(dropout_rate),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dense(1, name='layers_dense')\n",
    "                ])\n",
    "    else:\n",
    "        raise RuntimeError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745c0ecb",
   "metadata": {},
   "source": [
    "## Define method for training all testing models on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4444daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define testing function for generating and running tests\n",
    "def test(X, y, num_classes=None, X_test=None, y_test=None, task='classification', epochs=100, batch_size=32, task_name=''):\n",
    "    \n",
    "    MANN, MANN_drop, UANN1, UANN2, UANN3, UANN4, UANN5, SANN = None, None, None, None, None, None, None, None\n",
    "    \n",
    "    if X_test is None or y_test is None:\n",
    "        # Split data into training and testing\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "    else:\n",
    "        X_train, y_train = X, y\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "    train_dataset = train_dataset.shuffle(1000).batch(batch_size)\n",
    "    test_dataset = test_dataset.batch(batch_size)\n",
    "        \n",
    "    \n",
    "    if task == 'classification':\n",
    "        if num_classes is None:\n",
    "            raise RuntimeError()\n",
    "    \n",
    "        # Create our model\n",
    "        MANN = create_model(activations=[tf.nn.sigmoid, tf.nn.tanh, tf.nn.leaky_relu, tf.nn.elu, tf.nn.swish], dropout=False, num_classes=num_classes, task=task)\n",
    "        MANN.compile(optimizer=keras.optimizers.legacy.Nadam(),\n",
    "                     loss=keras.losses.CategoricalCrossentropy(),\n",
    "                     metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "\n",
    "        # Create our model with dropout\n",
    "        MANN_drop = create_model(activations=[tf.nn.sigmoid, tf.nn.tanh, tf.nn.leaky_relu, tf.nn.elu, tf.nn.swish], dropout=True, num_classes=num_classes, task=task)\n",
    "        MANN_drop.compile(optimizer=keras.optimizers.legacy.Nadam(),\n",
    "                         loss=keras.losses.CategoricalCrossentropy(),\n",
    "                         metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "\n",
    "        # Create uniform model for each activation function\n",
    "        UANN1 = create_model(activations=[tf.nn.sigmoid], option='uniform', num_classes=num_classes, task=task)\n",
    "        UANN1.compile(optimizer=keras.optimizers.legacy.Nadam(),\n",
    "                     loss=keras.losses.CategoricalCrossentropy(),\n",
    "                     metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "        \n",
    "        UANN2 = create_model(activations=[tf.nn.tanh], option='uniform', num_classes=num_classes, task=task)\n",
    "        UANN2.compile(optimizer=keras.optimizers.legacy.Nadam(),\n",
    "                     loss=keras.losses.CategoricalCrossentropy(),\n",
    "                     metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "        \n",
    "        UANN3 = create_model(activations=[tf.nn.leaky_relu], option='uniform', num_classes=num_classes, task=task)\n",
    "        UANN3.compile(optimizer=keras.optimizers.legacy.Nadam(),\n",
    "                     loss=keras.losses.CategoricalCrossentropy(),\n",
    "                     metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "        \n",
    "        UANN4 = create_model(activations=[tf.nn.elu], option='uniform', num_classes=num_classes, task=task)\n",
    "        UANN4.compile(optimizer=keras.optimizers.legacy.Nadam(),\n",
    "                     loss=keras.losses.CategoricalCrossentropy(),\n",
    "                     metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "        \n",
    "        UANN5 = create_model(activations=[tf.nn.swish], option='uniform', num_classes=num_classes, task=task)\n",
    "        UANN5.compile(optimizer=keras.optimizers.legacy.Nadam(),\n",
    "                     loss=keras.losses.CategoricalCrossentropy(),\n",
    "                     metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "        \n",
    "        # Create sequential activation network\n",
    "        SANN = create_model(activations=[tf.nn.swish, tf.nn.elu, tf.nn.leaky_relu, tf.nn.tanh, tf.nn.sigmoid], option='sequential', dropout=False, num_classes=num_classes, task=task)\n",
    "        SANN.compile(optimizer=keras.optimizers.legacy.Nadam(),\n",
    "                     loss=keras.losses.CategoricalCrossentropy(),\n",
    "                     metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "        \n",
    "    elif task == 'regression':\n",
    "        # Create our model\n",
    "        MANN = create_model(activations=[tf.nn.sigmoid, tf.nn.tanh, tf.nn.leaky_relu, tf.nn.elu, tf.nn.swish], dropout=False, task=task)\n",
    "        MANN.compile(optimizer=keras.optimizers.legacy.Nadam(),\n",
    "                     loss=keras.losses.MeanSquaredError())\n",
    "\n",
    "        # Create our model with dropout\n",
    "        MANN_drop = create_model(activations=[tf.nn.sigmoid, tf.nn.tanh, tf.nn.leaky_relu, tf.nn.elu, tf.nn.swish], dropout=True, task=task)\n",
    "        MANN_drop.compile(optimizer=keras.optimizers.legacy.Nadam(),\n",
    "                     loss=keras.losses.MeanSquaredError())\n",
    "        \n",
    "        # Create uniform model for each activation function\n",
    "        UANN1 = create_model(activations=[tf.nn.sigmoid], option='uniform', task=task)\n",
    "        UANN1.compile(optimizer=keras.optimizers.legacy.Nadam(),\n",
    "                     loss=keras.losses.MeanSquaredError())\n",
    "        \n",
    "        UANN2 = create_model(activations=[tf.nn.tanh], option='uniform', task=task)\n",
    "        UANN2.compile(optimizer=keras.optimizers.legacy.Nadam(),\n",
    "                     loss=keras.losses.MeanSquaredError())\n",
    "        \n",
    "        UANN3 = create_model(activations=[tf.nn.leaky_relu], option='uniform', task=task)\n",
    "        UANN3.compile(optimizer=keras.optimizers.legacy.Nadam(),\n",
    "                     loss=keras.losses.MeanSquaredError())\n",
    "        \n",
    "        UANN4 = create_model(activations=[tf.nn.elu], option='uniform', task=task)\n",
    "        UANN4.compile(optimizer=keras.optimizers.legacy.Nadam(),\n",
    "                     loss=keras.losses.MeanSquaredError())\n",
    "        \n",
    "        UANN5 = create_model(activations=[tf.nn.swish], option='uniform', task=task)\n",
    "        UANN5.compile(optimizer=keras.optimizers.legacy.Nadam(),\n",
    "                     loss=keras.losses.MeanSquaredError())\n",
    "        \n",
    "        # Create sequential activation network\n",
    "        SANN = create_model(activations=[tf.nn.swish, tf.nn.elu, tf.nn.leaky_relu, tf.nn.tanh, tf.nn.sigmoid], option='sequential', task=task)\n",
    "        SANN.compile(optimizer=keras.optimizers.legacy.Nadam(),\n",
    "                     loss=keras.losses.MeanSquaredError())\n",
    "    else:\n",
    "        raise RuntimeError()\n",
    "    \n",
    "    # Fit the data to the models for a set number of epochs\n",
    "    print(\"Training Multi Activation Neural Network...\")\n",
    "    path=f'{task_name}.MANN.h5'\n",
    "    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath=path,\n",
    "                                                                    save_weights_only=True,\n",
    "                                                                    monitor='val_loss',\n",
    "                                                                    mode='min',\n",
    "                                                                    save_best_only=True)\n",
    "    MANN_hist = MANN.fit(x=train_dataset, batch_size=batch_size, epochs=epochs, validation_data=test_dataset, callbacks=[model_checkpoint_callback])\n",
    "    print('')\n",
    "\n",
    "    print(\"Training Multi Activation Neural Network w/ Dropout...\")\n",
    "    path=f'{task_name}.MANN_dropout.h5'\n",
    "    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath=path,\n",
    "                                                                    save_weights_only=True,\n",
    "                                                                    monitor='val_loss',\n",
    "                                                                    mode='min',\n",
    "                                                                    save_best_only=True)\n",
    "    MANN_drop_hist = MANN_drop.fit(x=train_dataset, batch_size=batch_size, epochs=epochs, validation_data=test_dataset, callbacks=[model_checkpoint_callback])\n",
    "    print('Finished.')\n",
    "\n",
    "\n",
    "    print(\"Training Sigmoid Uniform Activation Neural Network...\")\n",
    "    path=f'{task_name}.Sigmoid.h5'\n",
    "    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath=path,\n",
    "                                                                    save_weights_only=True,\n",
    "                                                                    monitor='val_loss',\n",
    "                                                                    mode='min',\n",
    "                                                                    save_best_only=True)\n",
    "    UANN1_hist = UANN1.fit(x=train_dataset, batch_size=batch_size, epochs=epochs, validation_data=test_dataset, callbacks=[model_checkpoint_callback])\n",
    "    print('Finished.')\n",
    "\n",
    "    print(\"Training Tanh Uniform Activation Neural Network...\")\n",
    "    path=f'{task_name}.Tanh.h5'\n",
    "    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath=path,\n",
    "                                                                    save_weights_only=True,\n",
    "                                                                    monitor='val_loss',\n",
    "                                                                    mode='min',\n",
    "                                                                    save_best_only=True)\n",
    "    UANN2_hist = UANN2.fit(x=train_dataset, batch_size=batch_size, epochs=epochs, validation_data=test_dataset, callbacks=[model_checkpoint_callback])\n",
    "    print('Finished.')\n",
    "\n",
    "    print(\"Training Leaky ReLU Uniform Activation Neural Network...\")\n",
    "    path=f'{task_name}.LeakyReLU.h5'\n",
    "    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath=path,\n",
    "                                                                    save_weights_only=True,\n",
    "                                                                    monitor='val_loss',\n",
    "                                                                    mode='min',\n",
    "                                                                    save_best_only=True)\n",
    "    UANN3_hist = UANN3.fit(x=train_dataset, batch_size=batch_size, epochs=epochs, validation_data=test_dataset, callbacks=[model_checkpoint_callback])\n",
    "    print('Finished.')\n",
    "\n",
    "    print(\"Training ELU Uniform Activation Neural Network...\")\n",
    "    path=f'{task_name}.ELU.h5'\n",
    "    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath=path,\n",
    "                                                                    save_weights_only=True,\n",
    "                                                                    monitor='val_loss',\n",
    "                                                                    mode='min',\n",
    "                                                                    save_best_only=True)\n",
    "    UANN4_hist = UANN4.fit(x=train_dataset, batch_size=batch_size, epochs=epochs, validation_data=test_dataset, callbacks=[model_checkpoint_callback])\n",
    "    print('Finished.')\n",
    "\n",
    "    print(\"Training Swish Uniform Activation Neural Network...\")\n",
    "    path=f'{task_name}.Swish.h5'\n",
    "    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath=path,\n",
    "                                                                    save_weights_only=True,\n",
    "                                                                    monitor='val_loss',\n",
    "                                                                    mode='min',\n",
    "                                                                    save_best_only=True)\n",
    "    UANN5_hist = UANN5.fit(x=train_dataset, batch_size=batch_size, epochs=epochs, validation_data=test_dataset, callbacks=[model_checkpoint_callback])\n",
    "    print('Finished.')\n",
    "\n",
    "    print(\"Training Sequential Activation Neural Network...\")\n",
    "    path=f'{task_name}.Sequential.h5'\n",
    "    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath=path,\n",
    "                                                                    save_weights_only=True,\n",
    "                                                                    monitor='val_loss',\n",
    "                                                                    mode='min',\n",
    "                                                                    save_best_only=True)\n",
    "    SANN_hist = SANN.fit(x=train_dataset, batch_size=batch_size, epochs=epochs, validation_data=test_dataset, callbacks=[model_checkpoint_callback])\n",
    "    print('Finished.')\n",
    "\n",
    "    # Create dictionaries to return for models and scores\n",
    "    models = {\n",
    "        'MANN': MANN,\n",
    "        'MANN_Dropout': MANN_drop,\n",
    "        'Sigmoid': UANN1,\n",
    "        'Tanh': UANN2,\n",
    "        'LeakyReLU': UANN3,\n",
    "        'ELU': UANN4,\n",
    "        'Swish': UANN5,\n",
    "        'Sequential': SANN\n",
    "    }\n",
    "    \n",
    "    histories = {\n",
    "        'MANN': MANN_hist,\n",
    "        'MANN_dropout': MANN_drop_hist,\n",
    "        'Sigmoid': UANN1_hist,\n",
    "        'Tanh': UANN2_hist,\n",
    "        'LeakyReLU': UANN3_hist,\n",
    "        'ELU': UANN4_hist,\n",
    "        'Swish': UANN5_hist,\n",
    "        'Sequential': SANN_hist\n",
    "    }\n",
    "\n",
    "    return models, histories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52668e88",
   "metadata": {},
   "source": [
    "## Regression task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c3df3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and split into testing and training for processing\n",
    "data = pd.read_csv('./Datasets/Regression/train.csv')\n",
    "features = data.drop('TARGET(PRICE_IN_LACS)', axis=1)\n",
    "target = data['TARGET(PRICE_IN_LACS)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ec3512",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c903464",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a2e6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and normalize the data\n",
    "one_hot = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# One-Hot encode categorical columns\n",
    "X_train.append(pd.DataFrame(data=one_hot.fit_transform(X_train[['POSTED_BY', 'BHK_OR_RK']]), columns=one_hot.get_feature_names_out()))\n",
    "X_test.append(pd.DataFrame(data=one_hot.transform(X_test[['POSTED_BY', 'BHK_OR_RK']]), columns=one_hot.get_feature_names_out()))\n",
    "\n",
    "# Drop the transformed columns and unneeded columns\n",
    "X_train.drop(['POSTED_BY', 'BHK_OR_RK', 'ADDRESS'], axis = 1, inplace=True)\n",
    "X_test.drop(['POSTED_BY', 'BHK_OR_RK', 'ADDRESS'], axis=1, inplace=True)\n",
    "\n",
    "# Normalize numerical columns\n",
    "scaler = StandardScaler()\n",
    "X_train[['BHK_NO.', 'SQUARE_FT', 'LONGITUDE', 'LATITUDE']]= scaler.fit_transform(X_train[['BHK_NO.', 'SQUARE_FT', 'LONGITUDE', 'LATITUDE']])\n",
    "X_test[['BHK_NO.', 'SQUARE_FT', 'LONGITUDE', 'LATITUDE']] = scaler.transform(X_test[['BHK_NO.', 'SQUARE_FT', 'LONGITUDE', 'LATITUDE']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fffed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf71e787",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create and fit all neural networks for evaluations\n",
    "models, histories = test(X_train.to_numpy(), y_train.to_numpy(), X_test=X_test.to_numpy(), y_test=y_test.to_numpy(), task='regression', epochs=500, batch_size=128, task_name='Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01f347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss over the epochs\n",
    "plt.plot(np.array(range(len(histories['MANN'].history['loss']))), histories['MANN'].history['loss'], label='MANN')\n",
    "plt.plot(np.array(range(len(histories['MANN_dropout'].history['loss']))), histories['MANN_dropout'].history['loss'], label='MANN_dropout')\n",
    "plt.plot(np.array(range(len(histories['Sigmoid'].history['loss']))), histories['Sigmoid'].history['loss'], label='Sigmoid')\n",
    "plt.plot(np.array(range(len(histories['Tanh'].history['loss']))), histories['Tanh'].history['loss'], label='Tanh')\n",
    "plt.plot(np.array(range(len(histories['LeakyReLU'].history['loss']))), histories['LeakyReLU'].history['loss'], label='Leaky ReLU')\n",
    "plt.plot(np.array(range(len(histories['ELU'].history['loss']))), histories['ELU'].history['loss'], label='ELU')\n",
    "plt.plot(np.array(range(len(histories['Swish'].history['loss']))), histories['Swish'].history['loss'], label='Swish')\n",
    "plt.plot(np.array(range(len(histories['Sequential'].history['loss']))), histories['Sequential'].history['loss'], label='Sequential')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c132234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the validation loss over the epochs\n",
    "plt.plot(np.array(range(len(histories['MANN'].history['loss']))), histories['MANN'].history['val_loss'], label='MANN', linewidth=0.75)\n",
    "plt.plot(np.array(range(len(histories['MANN_dropout'].history['loss']))), histories['MANN_dropout'].history['val_loss'], label='MANN_dropout', linewidth=0.75)\n",
    "plt.plot(np.array(range(len(histories['Sigmoid'].history['loss']))), histories['Sigmoid'].history['val_loss'], label='Sigmoid', linewidth=0.75)\n",
    "plt.plot(np.array(range(len(histories['Tanh'].history['loss']))), histories['Tanh'].history['val_loss'], label='Tanh', linewidth=0.75)\n",
    "plt.plot(np.array(range(len(histories['LeakyReLU'].history['loss']))), histories['LeakyReLU'].history['val_loss'], label='Leaky ReLU', linewidth=0.75)\n",
    "plt.plot(np.array(range(len(histories['ELU'].history['loss']))), histories['ELU'].history['val_loss'], label='ELU', linewidth=0.75)\n",
    "plt.plot(np.array(range(len(histories['Swish'].history['loss']))), histories['Swish'].history['val_loss'], label='Swish', linewidth=0.75)\n",
    "plt.plot(np.array(range(len(histories['Sequential'].history['loss']))), histories['Sequential'].history['val_loss'], label='Sequential', linewidth=0.75)\n",
    "plt.legend(bbox_to_anchor=(1.1, 1.05))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde53abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history\n",
    "train_hist_df = pd.DataFrame()\n",
    "val_hist_df = pd.DataFrame()\n",
    "for name, callback in histories.items():\n",
    "    train_hist_df = pd.concat((train_hist_df, pd.DataFrame(callback.history['loss'], columns=[name])), axis=1, join='inner', ignore_index=True)\n",
    "    val_hist_df = pd.concat((val_hist_df, pd.DataFrame(callback.history['val_loss'], columns=[name])), axis=1, join='inner', ignore_index=True)\n",
    "    \n",
    "train_hist_df.to_csv('Regression.training_hist.csv')\n",
    "val_hist_df.to_csv('Regression.validation_hist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87203ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best performing weights for each model and fit the traditional ML models\n",
    "for name, model in models.items():\n",
    "    model.load_weights(f'Regression.{name}.tf')\n",
    "\n",
    "# Traditional models as a baseline comparison\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "svm = LinearSVR(max_iter=10000)\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa5c4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the testing loss for each model\n",
    "linreg_loss = keras.losses.mean_squared_error(y_test, linreg.predict(X_test)).numpy()\n",
    "svm_loss = keras.losses.mean_squared_error(y_test, svm.predict(X_test)).numpy()\n",
    "MANN_loss = models['MANN'].evaluate(X_test, y_test)\n",
    "MANN_dropout_loss = models['MANN_Dropout'].evaluate(X_test, y_test)\n",
    "Sigmoid_loss = models['Sigmoid'].evaluate(X_test, y_test)\n",
    "Tanh_loss = models['Tanh'].evaluate(X_test, y_test)\n",
    "LeakyReLU_loss = models['LeakyReLU'].evaluate(X_test, y_test)\n",
    "ELU_loss = models['ELU'].evaluate(X_test, y_test)\n",
    "Swish_loss = models['Swish'].evaluate(X_test, y_test)\n",
    "Sequential_loss = models['Sequential'].evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611db66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save testing loss to table and display results\n",
    "results = pd.DataFrame([linreg_loss, svm_loss, MANN_loss, MANN_dropout_loss, Sigmoid_loss, Tanh_loss, LeakyReLU_loss, ELU_loss, Swish_loss, Sequential_loss],\n",
    "                      index=['Linear Regression', 'Support Vector Machine', 'MANN', 'MANN w/ Dropout', 'Sigmoid NN', 'Tanh NN', 'Leaky ReLU NN', 'ELU NN', 'Swish NN', 'Sequential NN'],\n",
    "                      columns=['Mean Squared Error'])\n",
    "results.sort_values('Mean Squared Error', inplace=True)\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d593718",
   "metadata": {},
   "source": [
    "## NLP task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c873f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and split into testing and training for processing\n",
    "data = pd.read_csv('./Datasets/NLP/twitter_MBTI.csv', index_col=0)\n",
    "features = data['text']\n",
    "target = data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.20, random_state=42, stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d711e73e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7811</td>\n",
       "      <td>7811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>7581</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>I just realize that today is dzi's birthday......</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>1282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text label\n",
       "count                                                7811  7811\n",
       "unique                                               7581    16\n",
       "top     I just realize that today is dzi's birthday......  infp\n",
       "freq                                                    2  1282"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "727ced36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@Pericles216 @HierBeforeTheAC @Sachinettiyil T...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Hispanthicckk Being you makes you look cute||...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Alshymi Les balles sont réelles et sont tirée...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm like entp but idiotic|||Hey boy, do you wa...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@kaeshurr1 Give it to @ZargarShanif ... He has...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Doing my project on isle neumann|||@fuzy_sox I...</td>\n",
       "      <td>entp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>she gets it https://t.co/tK0PlXKO2d|||@nakopoc...</td>\n",
       "      <td>entp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>@WeezingOffline @Max__4__x @raptalksk @Hemant3...</td>\n",
       "      <td>entp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Thanks to @About1816 for inviting me to write ...</td>\n",
       "      <td>entp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Nichkhun IG Post 2022.06.19\\nExtra with 2AM Jo...</td>\n",
       "      <td>entp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text label\n",
       "0   @Pericles216 @HierBeforeTheAC @Sachinettiyil T...  intj\n",
       "1   @Hispanthicckk Being you makes you look cute||...  intj\n",
       "2   @Alshymi Les balles sont réelles et sont tirée...  intj\n",
       "3   I'm like entp but idiotic|||Hey boy, do you wa...  intj\n",
       "4   @kaeshurr1 Give it to @ZargarShanif ... He has...  intj\n",
       "..                                                ...   ...\n",
       "95  Doing my project on isle neumann|||@fuzy_sox I...  entp\n",
       "96  she gets it https://t.co/tK0PlXKO2d|||@nakopoc...  entp\n",
       "97  @WeezingOffline @Max__4__x @raptalksk @Hemant3...  entp\n",
       "98  Thanks to @About1816 for inviting me to write ...  entp\n",
       "99  Nichkhun IG Post 2022.06.19\\nExtra with 2AM Jo...  entp\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb79c148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def lemmatize(document):\n",
    "    word_pos_tags = nltk.pos_tag(document)\n",
    "    roots = [lemmatizer.lemmatize(tag[0], wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)]\n",
    "    return roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7283a6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Process and normalize the data\n",
    "one_hot = OneHotEncoder(sparse_output=False)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# One-Hot encode target labels\n",
    "y_train = pd.DataFrame(data=one_hot.fit_transform(y_train.to_numpy().reshape(-1,1)), columns=one_hot.categories_)\n",
    "y_test = pd.DataFrame(data=one_hot.transform(y_test.to_numpy().reshape(-1,1)), columns=one_hot.categories_)\n",
    "\n",
    "# Clean and process text\n",
    "X_train = X_train.map(lambda x: x.lower().strip())\n",
    "X_test = X_test.map(lambda x: x.lower().strip())\n",
    "\n",
    "X_train = X_train.map(lambda x: [token for y in x.split('|||') for token in y.split(' ') if 'http' not in token and '@' not in token])\n",
    "X_test = X_test.map(lambda x: [token for y in x.split('|||') for token in y.split(' ') if 'http' not in token and '@' not in token])\n",
    "\n",
    "X_train = X_train.map(lambda x: ' '.join(x))\n",
    "X_test = X_test.map(lambda x: ' '.join(x))\n",
    "\n",
    "X_train = X_train.map(word_tokenize)\n",
    "X_test = X_test.map(word_tokenize)\n",
    "\n",
    "X_train = X_train.map(lambda x: [word for word in x if word not in stop_words and word.isalpha()])\n",
    "X_test = X_test.map(lambda x: [word for word in x if word not in stop_words and word.isalpha()])\n",
    "\n",
    "X_train = X_train.map(lambda x: lemmatize(x))\n",
    "X_test = X_test.map(lambda x: lemmatize(x))\n",
    "\n",
    "X_train = X_train.map(lambda x: ' '.join(x))\n",
    "X_test = X_test.map(lambda x: ' '.join(x))\n",
    "\n",
    "# Create word embeddings with TF-IDF\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "X_train = np.asarray(X_train.todense())\n",
    "X_test = np.asarray(X_test.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a43ae507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179089"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.TextVectorization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e08b35aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create and fit all neural networks for evaluations\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m models, histories \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mone_hot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategories_\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclassification\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNLP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 12\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(X, y, num_classes, X_test, y_test, task, epochs, batch_size, task_name)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m     X_train, y_train \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m---> 12\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices((X_test, y_test))\n\u001b[0;32m     15\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;241m1000\u001b[39m)\u001b[38;5;241m.\u001b[39mbatch(batch_size)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:814\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_tensor_slices\u001b[39m(tensors, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    738\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a `Dataset` whose elements are slices of the given tensors.\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \n\u001b[0;32m    740\u001b[0m \u001b[38;5;124;03m  The given tensors are sliced along their first dimension. This operation\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;124;03m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 814\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTensorSliceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4708\u001b[0m, in \u001b[0;36mTensorSliceDataset.__init__\u001b[1;34m(self, element, is_files, name)\u001b[0m\n\u001b[0;32m   4706\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, element, is_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   4707\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"See `Dataset.from_tensor_slices()` for details.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 4708\u001b[0m   element \u001b[38;5;241m=\u001b[39m \u001b[43mstructure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4709\u001b[0m   batched_spec \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mtype_spec_from_value(element)\n\u001b[0;32m   4710\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_batched_tensor_list(batched_spec, element)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:126\u001b[0m, in \u001b[0;36mnormalize_element\u001b[1;34m(element, element_signature)\u001b[0m\n\u001b[0;32m    123\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(spec, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    125\u001b[0m         normalized_components\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 126\u001b[0m             \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomponent_\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mpack_sequence_as(pack_as, normalized_components)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1638\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1629\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1630\u001b[0m           _add_error_prefix(\n\u001b[0;32m   1631\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1634\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1635\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m   1637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1638\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1640\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m   1641\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py:48\u001b[0m, in \u001b[0;36m_default_conversion_function\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_default_conversion_function\u001b[39m(value, dtype, name, as_ref):\n\u001b[0;32m     47\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m as_ref  \u001b[38;5;66;03m# Unused.\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    172\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:279\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    278\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m--> 279\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m g \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mget_default_graph()\n\u001b[0;32m    282\u001b[0m tensor_value \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:304\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[0;32m    303\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 304\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "# Create and fit all neural networks for evaluations\n",
    "models, histories = test(X_train, y_train.to_numpy(), X_test=X_test, y_test=y_test.to_numpy(), num_classes=len(one_hot.categories_[0]), task='classification', epochs=500, batch_size=1, task_name='NLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29664b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss over the epochs\n",
    "plt.plot(np.array(range(len(histories['MANN'].history['loss']))), histories['MANN'].history['loss'], label='MANN')\n",
    "plt.plot(np.array(range(len(histories['MANN_dropout'].history['loss']))), histories['MANN_dropout'].history['loss'], label='MANN_dropout')\n",
    "plt.plot(np.array(range(len(histories['Sigmoid'].history['loss']))), histories['Sigmoid'].history['loss'], label='Sigmoid')\n",
    "plt.plot(np.array(range(len(histories['Tanh'].history['loss']))), histories['Tanh'].history['loss'], label='Tanh')\n",
    "plt.plot(np.array(range(len(histories['LeakyReLU'].history['loss']))), histories['LeakyReLU'].history['loss'], label='Leaky ReLU')\n",
    "plt.plot(np.array(range(len(histories['ELU'].history['loss']))), histories['ELU'].history['loss'], label='ELU')\n",
    "plt.plot(np.array(range(len(histories['Swish'].history['loss']))), histories['Swish'].history['loss'], label='Swish')\n",
    "plt.plot(np.array(range(len(histories['Sequential'].history['loss']))), histories['Sequential'].history['loss'], label='Sequential')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec42fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the validation loss over the epochs\n",
    "plt.plot(np.array(range(len(histories['MANN'].history['loss']))), histories['MANN'].history['val_loss'], label='MANN', linewidth=0.75)\n",
    "plt.plot(np.array(range(len(histories['MANN_dropout'].history['loss']))), histories['MANN_dropout'].history['val_loss'], label='MANN_dropout', linewidth=0.75)\n",
    "plt.plot(np.array(range(len(histories['Sigmoid'].history['loss']))), histories['Sigmoid'].history['val_loss'], label='Sigmoid', linewidth=0.75)\n",
    "plt.plot(np.array(range(len(histories['Tanh'].history['loss']))), histories['Tanh'].history['val_loss'], label='Tanh', linewidth=0.75)\n",
    "plt.plot(np.array(range(len(histories['LeakyReLU'].history['loss']))), histories['LeakyReLU'].history['val_loss'], label='Leaky ReLU', linewidth=0.75)\n",
    "plt.plot(np.array(range(len(histories['ELU'].history['loss']))), histories['ELU'].history['val_loss'], label='ELU', linewidth=0.75)\n",
    "plt.plot(np.array(range(len(histories['Swish'].history['loss']))), histories['Swish'].history['val_loss'], label='Swish', linewidth=0.75)\n",
    "plt.plot(np.array(range(len(histories['Sequential'].history['loss']))), histories['Sequential'].history['val_loss'], label='Sequential', linewidth=0.75)\n",
    "plt.legend(bbox_to_anchor=(1.1, 1.05))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ea1ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history\n",
    "train_hist_df = pd.DataFrame()\n",
    "val_hist_df = pd.DataFrame()\n",
    "for name, callback in histories.items():\n",
    "    train_hist_df = pd.concat(hist_df, pd.DataFrame(callback.history['loss'], columns=[name]), axis=1, join='inner', ignore_index=True)\n",
    "    val_hist_df = pd.concat(hist_df, pd.DataFrame(callback.history['val_loss'], columns=[name]), axis=1, join='inner', ignore_index=True)\n",
    "    \n",
    "train_hist_df.to_csv('NLP.training_hist.csv')\n",
    "val_his_df.to_csv('NLP.validation_hist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4220b853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best performing weights for each model and fit the traditional ML models\n",
    "for name, model in models.items():\n",
    "    model.load_weights(f'NLP.{name}.tf')\n",
    "    \n",
    "# Traditional Models as a baseline comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f19973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get performance metrics for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef8fb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics to a table and display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d4b8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual results with confusion matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
